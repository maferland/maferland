{
  "title": "How to generate your sitemap.xml for Next.js and Netlify CMS",
  "slug": "/sitemap-nextjs-netlify-cms",
  "date": "2020-05-20T01:23:12.969Z",
  "image": {
    "url": "/img/book-maps-explorer.jpg",
    "alt": "flat ray photography of book, pencil, camera, and with lens"
  },
  "description": "There are many steps to improve the SEO ranking of your website. Having a sitemap cannot hurt. Here is a method to generate a proper and up-to-date sitemap for a Next.js website powered by Netlify CMS.",
  "body": "When I started to look for `nextjs generate sitemap.xml`, I was slightly disapointed. Maybe because I remember how easy it was to setup while using Nuxt.js. But mostly because most solution seemed either overkill (with a lot of configuration) or on the other hand a bit too bare metal and naive.\n\nOdly enough, I decided to build a fully custom solution. [Martin Beierling-Mutz](https://dev.to/embiem) posted an article that is very close to what I need so I edited his [solution](https://embiem.me/blog/auto-generate-sitemap-in-next-js/) to meet my needs.\n\n## List the content\n\nMy website being very simple I only have a handfull of pages.\n\n* / (the home page)\n\n* /about\n\n* /blog\n\nThe most important part was to be able to automatically add new blog posts to the sitemap.\n\nI use [Netlify CMS](https://www.netlifycms.org/) to write control the content of my website. It works very for simple static site generator. The logic of this git-based CMS is that all post/page/etc. are commited in the repo. Whenever there's a new commit the website is built and deployed again.\n\nThis mean there is no API to query to know the current articles available on the website. Martin's solution tried to solve this issue by listing the files found in the `/pages/` directory.\n\nSince my articles are already in the repo I figured I could use that instead. I use custom slug in configuration so I have to read everyfile to define it's route. I can't rely on the file name. This is a bit of a drawback of my solution compared to Martin's. But, it's already online as I write this ðŸ™ˆ.\n\n```jsx\npaths.push(\n  ...fs.readdirSync('site/blog/').map((file) => {\n    const rawfile = fs.readFileSync(`site/blog/${file}`);\n    const post = JSON.parse(rawfile);\n    return `/blog/${post.slug}`;\n  }),\n);\n```\n\n## Writting the file on disk\n\nMy version only relies on newer `fs` features that weren't available (I think) when the original post was written. The files have to be at the root of the website. In my case, the output folder id `/out`;\n\nThis way, I can write the `robots.txt` like this\n\n```js\nconst robotsTxt = `User-agent: *\nSitemap: https://maferland.com/sitemap.xml\nDisallow:`;\n\nfs.writeFile('out/robots.txt', robotsTxt, () => { \n  console.log('robots.txt saved!');\n});\n```\n\nThen, I write the sitemap.xml like this\n\n```js\nlet xmlPaths = '';\n\npaths\n  .map(\n    (path) => `\n  <url>\n    <loc>https://maferland.com${path}</loc>\n  </url>`,\n  )\n  .forEach((xmlPath) => {\n    xmlPaths += xmlPath;\n  });\n\nconst sitemapXml = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">${xmlPaths}\n</urlset>`;\n\nfs.writeFile('out/sitemap.xml', sitemapXml, () => {\n  console.log('sitemap.xml saved!');\n});\n```\n\n## Conclusion\n\nI think writting this helped me a bit figure out the orginal post solution. You simply have [the full script](https://github.com/maferland/maferland/commit/fae200bb00043576c402f0b95829eb74bab9c17b) as a `postexport` step.\n\nWhenever your website will be deployed again this will be ran again. If your website is built using a flat-file cms this should provide an alway up-to-date sitemap.xml for Google to crawl ðŸ™ƒ.\n\nI will probably write another version of this script very soon to apply my newly acquired knowledge. Don't copy paste it without asking yourself if it's good enough for your use case!"
}